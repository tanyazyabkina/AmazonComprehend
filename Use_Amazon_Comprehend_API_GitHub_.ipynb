{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon Comprehend Through the boto3 API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use boto3 Amazon API to use Amazon Comprehend for real time analysis as well as scheduling analysis jobs.\n",
    "1. For boto3 to work you need to create an IAM User, receive `aws_access_key_id` and `aws_secret_access_key` and configure your credentials using AWS Command Line Interface (AWS CLI).\n",
    "2. Cost. If you are using free AWS tier, you can analyze 50K units a month free. In my example, every tweet is a unit. In the scheduled job I am analyzing 10K tweets at once, so the free tier runs out pretty fast, and then it's \\$1 per 10K. Be sure to check pricing before you proceed. https://aws.amazon.com/comprehend/pricing/\n",
    "3. Reference. Boto3 S3: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html  Boto3 Comprehend: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "import json\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference to S3 Upload and Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name  = 'comprehend-api'\n",
    "local_file_name = 'Comprehend/amazon_tweets.csv'\n",
    "aws_file_name = 'input-data/amazon_tweets.csv'\n",
    "# Upload file to specific location\n",
    "upload_file(local_file_name, bucket_name, aws_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_results_S3Url = 's3://BUCKETNAME/results/LONG_LONG_NAME/output/output.tar.gz'\n",
    "local_results_filename = 'Comprehend/outputs/entities.tar.gz'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3_name = 's3://' + bucket_name + '/'\n",
    "results_aws_filename = entities_results_S3Url.replace(s3_name, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "s3.download_file(bucket_name,\n",
    "                 results_aws_filename, \n",
    "                 local_results_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targz(targz_file, output_path = ''):\n",
    "    if targz_file.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(targz_file, \"r:gz\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "    elif targz_file.endswith(\"tar\"):\n",
    "        tar = tarfile.open(targz_file, \"r:\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'Comprehend/outputs/extracted'\n",
    "extract_targz(local_results_filename, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Single Record Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_name = 'Comprehend/amazon_tweets.csv'\n",
    "region_name = 'us-east-2' # USE YOUR REGION HERE\n",
    "# read data\n",
    "df = pd.read_csv(local_file_name, header = None, names = ['amazon_tweets'], dtype = 'str')\n",
    "# Record to examine\n",
    "df.loc[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize comprehend module\n",
    "comprehend = boto3.client(service_name='comprehend', region_name=region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'NEUTRAL',\n",
       " 'SentimentScore': {'Positive': 0.28640827536582947,\n",
       "  'Negative': 0.0001400432811351493,\n",
       "  'Neutral': 0.7134460806846619,\n",
       "  'Mixed': 5.648644219036214e-06},\n",
       " 'ResponseMetadata': {'RequestId': '40eb6a43-8745-4cf6-ab4d-884dce7fd680',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '40eb6a43-8745-4cf6-ab4d-884dce7fd680',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '163',\n",
       "   'date': 'Tue, 29 Sep 2020 18:03:53 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run sentiment analysis\n",
    "sentiment_output = comprehend.detect_sentiment(Text=df.loc[0].item(), LanguageCode='en')\n",
    "# Output\n",
    "sentiment_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Positive': 0.28640827536582947,\n",
       " 'Negative': 0.0001400432811351493,\n",
       " 'Neutral': 0.7134460806846619,\n",
       " 'Mixed': 5.648644219036214e-06}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score\n",
    "sentiment_output['SentimentScore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Multiple Record Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A maximum of 25 records can be analyzed\n",
    "tweets25 = list(df.amazon_tweets[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a sentiment batch\n",
    "sentiment_batch = comprehend.batch_detect_sentiment(TextList=tweets25,\n",
    "                                                    LanguageCode='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check a tweet and its sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@thom_galvin https://t.co/hLLKVH322N Book to soon to come! Getting excited!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets25[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Index': 4,\n",
       " 'Sentiment': 'POSITIVE',\n",
       " 'SentimentScore': {'Positive': 0.9799827337265015,\n",
       "  'Negative': 6.78108845022507e-05,\n",
       "  'Neutral': 0.019944585859775543,\n",
       "  'Mixed': 4.9112672968476545e-06}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_batch['ResultList'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentiment_batch(data):\n",
    "    df = pd.DataFrame() \n",
    "    for line in data['ResultList']:\n",
    "        try:\n",
    "            dt_temp = pd.DataFrame(line['SentimentScore'], index = [0])  # extract data from sub-dictionary\n",
    "            for field in list(line.keys())[:-1]:  # add common fields\n",
    "                dt_temp[field] = line[field]\n",
    "        \n",
    "            df = df.append(dt_temp, ignore_index = True)\n",
    "                  \n",
    "        except:\n",
    "            for field in list(line.keys())[:-1]:  # add common fields\n",
    "                dt_temp[field] = line[field]\n",
    "        \n",
    "            df = df.append(dt_temp, ignore_index = True)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Mixed</th>\n",
       "      <th>Index</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.286408</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.713446</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015133</td>\n",
       "      <td>0.361038</td>\n",
       "      <td>0.619455</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>1</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.991953</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.747230</td>\n",
       "      <td>0.250880</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.979983</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.019945</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Positive  Negative   Neutral     Mixed  Index Sentiment\n",
       "0  0.286408  0.000140  0.713446  0.000006      0   NEUTRAL\n",
       "1  0.015133  0.361038  0.619455  0.004373      1   NEUTRAL\n",
       "2  0.000617  0.991953  0.007428  0.000002      2  NEGATIVE\n",
       "3  0.001871  0.747230  0.250880  0.000019      3  NEGATIVE\n",
       "4  0.979983  0.000068  0.019945  0.000005      4  POSITIVE"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_sentiment_batch(sentiment_batch).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entities Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run entities analysis on a batch\n",
    "entities_batch = comprehend.batch_detect_entities(TextList=tweets25, LanguageCode='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://t.co/L7HVYLQu9Q We\\xa0look forward to connecting with you through the camera tonight at 7pm CT! \\xa0 Here are the ways to watch: Watch at https://t.co/kcNTEKjuSU or via Facebook Live, Roku or Amazon Fire https://t.co/QdUkItoUlT'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample tweet\n",
    "tweets25[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>BeginOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.987823</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/L7HVYLQu9Q</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.868062</td>\n",
       "      <td>DATE</td>\n",
       "      <td>tonight at</td>\n",
       "      <td>82</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.869554</td>\n",
       "      <td>DATE</td>\n",
       "      <td>7pm CT</td>\n",
       "      <td>93</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992966</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/kcNTEKjuSU</td>\n",
       "      <td>140</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.658535</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>Facebook Live</td>\n",
       "      <td>171</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.956544</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Roku</td>\n",
       "      <td>186</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.966188</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>194</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.954848</td>\n",
       "      <td>COMMERCIAL_ITEM</td>\n",
       "      <td>Fire</td>\n",
       "      <td>201</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991052</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/QdUkItoUlT</td>\n",
       "      <td>206</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score             Type                     Text  BeginOffset  EndOffset\n",
       "0  0.987823            OTHER  https://t.co/L7HVYLQu9Q            0         23\n",
       "1  0.868062             DATE               tonight at           82         92\n",
       "2  0.869554             DATE                   7pm CT           93         99\n",
       "3  0.992966            OTHER  https://t.co/kcNTEKjuSU          140        163\n",
       "4  0.658535            TITLE            Facebook Live          171        184\n",
       "5  0.956544     ORGANIZATION                     Roku          186        190\n",
       "6  0.966188     ORGANIZATION                   Amazon          194        200\n",
       "7  0.954848  COMMERCIAL_ITEM                     Fire          201        205\n",
       "8  0.991052            OTHER  https://t.co/QdUkItoUlT          206        229"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entities in a sample tweet\n",
    "pd.DataFrame(entities_batch['ResultList'][0]['Entities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse entities into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the dictionary\n",
    "def parse_entities_batch(data):\n",
    "    df = pd.DataFrame() # declare an empty dataframe\n",
    "    nested_json = 'Entities' # nested sub-dictiptionary to extract data from\n",
    "    # populate the dataframe\n",
    "    for line in data['ResultList']:\n",
    "        dt_temp = pd.DataFrame(line[nested_json])  # extract data from sub-dictionary\n",
    "        other_fields = list(line.keys())\n",
    "        other_fields.remove(nested_json) # remove nested fields        \n",
    "        for field in other_fields:  # add common fields\n",
    "            dt_temp[field] = line[field]\n",
    "        \n",
    "        df = df.append(dt_temp, ignore_index = True)\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>BeginOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.987823</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/L7HVYLQu9Q</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.868062</td>\n",
       "      <td>DATE</td>\n",
       "      <td>tonight at</td>\n",
       "      <td>82</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.869554</td>\n",
       "      <td>DATE</td>\n",
       "      <td>7pm CT</td>\n",
       "      <td>93</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992966</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/kcNTEKjuSU</td>\n",
       "      <td>140</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.658535</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>Facebook Live</td>\n",
       "      <td>171</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.956544</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Roku</td>\n",
       "      <td>186</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.966188</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>194</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.954848</td>\n",
       "      <td>COMMERCIAL_ITEM</td>\n",
       "      <td>Fire</td>\n",
       "      <td>201</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991052</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/QdUkItoUlT</td>\n",
       "      <td>206</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.597910</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>@JediLive</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.960665</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>@SmellMyBert</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.292009</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>@XraypadOfficial</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.915255</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>about $83</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.992801</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.988215</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>$86</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score             Type                     Text  BeginOffset  \\\n",
       "0   0.987823            OTHER  https://t.co/L7HVYLQu9Q            0   \n",
       "1   0.868062             DATE               tonight at           82   \n",
       "2   0.869554             DATE                   7pm CT           93   \n",
       "3   0.992966            OTHER  https://t.co/kcNTEKjuSU          140   \n",
       "4   0.658535            TITLE            Facebook Live          171   \n",
       "5   0.956544     ORGANIZATION                     Roku          186   \n",
       "6   0.966188     ORGANIZATION                   Amazon          194   \n",
       "7   0.954848  COMMERCIAL_ITEM                     Fire          201   \n",
       "8   0.991052            OTHER  https://t.co/QdUkItoUlT          206   \n",
       "9   0.597910           PERSON                @JediLive            0   \n",
       "10  0.960665           PERSON             @SmellMyBert           10   \n",
       "11  0.292009           PERSON         @XraypadOfficial           23   \n",
       "12  0.915255         QUANTITY                about $83           40   \n",
       "13  0.992801     ORGANIZATION                   Amazon           53   \n",
       "14  0.988215         QUANTITY                      $86           70   \n",
       "\n",
       "    EndOffset  Index  \n",
       "0          23      0  \n",
       "1          92      0  \n",
       "2          99      0  \n",
       "3         163      0  \n",
       "4         184      0  \n",
       "5         190      0  \n",
       "6         200      0  \n",
       "7         205      0  \n",
       "8         229      0  \n",
       "9           9      1  \n",
       "10         22      1  \n",
       "11         39      1  \n",
       "12         49      1  \n",
       "13         59      1  \n",
       "14         73      1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_batch_df = parse_entities_batch(entities_batch)\n",
    "entities_batch_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all 25 tweets\n",
    "tweet_dump = [''.join(tweets25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Key Phrases batch\n",
    "key_phrases_batch_output = comprehend.batch_detect_key_phrases(TextList=tweet_dump, LanguageCode='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>BeginOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.809662</td>\n",
       "      <td>the camera tonight</td>\n",
       "      <td>71</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999997</td>\n",
       "      <td>7pm CT</td>\n",
       "      <td>93</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999996</td>\n",
       "      <td>the ways</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.735539</td>\n",
       "      <td>Watch</td>\n",
       "      <td>131</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.949925</td>\n",
       "      <td>Facebook Live, Roku or Amazon Fire https://t.c...</td>\n",
       "      <td>171</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.999983</td>\n",
       "      <td>the twilight movies</td>\n",
       "      <td>3984</td>\n",
       "      <td>4003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.845206</td>\n",
       "      <td>their site BUT HULU ADDED THEM YOU</td>\n",
       "      <td>4008</td>\n",
       "      <td>4042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.963395</td>\n",
       "      <td>KNOW WTF GOIN ONhaven</td>\n",
       "      <td>4051</td>\n",
       "      <td>4072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.913137</td>\n",
       "      <td>amazon</td>\n",
       "      <td>4095</td>\n",
       "      <td>4101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.999989</td>\n",
       "      <td>the last 21 days</td>\n",
       "      <td>4105</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Score                                               Text  BeginOffset  \\\n",
       "0    0.809662                                 the camera tonight           71   \n",
       "1    0.999997                                             7pm CT           93   \n",
       "2    0.999996                                           the ways          112   \n",
       "3    0.735539                                              Watch          131   \n",
       "4    0.949925  Facebook Live, Roku or Amazon Fire https://t.c...          171   \n",
       "..        ...                                                ...          ...   \n",
       "118  0.999983                                the twilight movies         3984   \n",
       "119  0.845206                 their site BUT HULU ADDED THEM YOU         4008   \n",
       "120  0.963395                              KNOW WTF GOIN ONhaven         4051   \n",
       "121  0.913137                                             amazon         4095   \n",
       "122  0.999989                                   the last 21 days         4105   \n",
       "\n",
       "     EndOffset  \n",
       "0           89  \n",
       "1           99  \n",
       "2          120  \n",
       "3          136  \n",
       "4          251  \n",
       "..         ...  \n",
       "118       4003  \n",
       "119       4042  \n",
       "120       4072  \n",
       "121       4101  \n",
       "122       4121  \n",
       "\n",
       "[123 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results\n",
    "pd.DataFrame(key_phrases_batch_output['ResultList'][0]['KeyPhrases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling an Analysis Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly recommend that you run at least one Comprehend job from the point and click interface, especially, if you are new to AWS. This way you can create a data access role (aka `data_access_role_arn`), and then you can simply copy the role name from the job description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name  = 'comprehend-api'\n",
    "local_file_name = 'Comprehend/amazon_tweets.csv'\n",
    "aws_file_name = 'input-data/amazon_tweets.csv'\n",
    "# Upload file to specific location\n",
    "upload_file(local_file_name, bucket_name, aws_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these values before running the program\n",
    "input_s3_url = 's3://comprehend-api/input-data'\n",
    "input_doc_format = 'ONE_DOC_PER_LINE'\n",
    "output_s3_url = 's3://comprehend-api/results'\n",
    "data_access_role_arn = \"arn:aws:iam::XXXXXXXXX:role/service-role/YOUR_ROLE_NAME\"\n",
    "number_of_topics = 10   # Optional argument\n",
    "\n",
    "# Set up job configuration\n",
    "input_data_config = {'S3Uri': input_s3_url, 'InputFormat': input_doc_format}\n",
    "output_data_config = {'S3Uri': output_s3_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Topic Detection Job: 48a33dda99b62aec30273a0d757a3d90\n"
     ]
    }
   ],
   "source": [
    "# Begin a job to detect the topics in the document collection\n",
    "comprehend = boto3.client('comprehend')\n",
    "start_job_entities = comprehend.start_entities_detection_job(\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    DataAccessRoleArn=data_access_role_arn,\n",
    "    LanguageCode='en')\n",
    "job_id = start_job_entities['JobId']\n",
    "print(f'Started Topic Detection Job: {job_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Retrieve information about the job\n",
    "describe_result = comprehend.describe_entities_detection_job(JobId=job_id)\n",
    "job_status = describe_result['EntitiesDetectionJobProperties']['JobStatus']\n",
    "print(f'Job Status: {job_status}')\n",
    "if job_status == 'FAILED':\n",
    "    print(f'Reason: {describe_result[\"EntitiesDetectionJobProperties\"][\"Message\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 3332908855ffce7a71524805a6b9f6ad, Status: COMPLETED\n",
      "Job ID: 54605b617b2e0c8fedc67962130dc3cc, Status: COMPLETED\n",
      "Job ID: cd1b44d1a6f1db713f96298b5987c716, Status: COMPLETED\n",
      "Job ID: 1a5a38d9a5c02ee15ed806675271a034, Status: COMPLETED\n",
      "Job ID: cc651ab875e5e50eb476bfac7b424cb7, Status: COMPLETED\n",
      "Job ID: 48a33dda99b62aec30273a0d757a3d90, Status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# List all topic-detection jobs\n",
    "list_result = comprehend.list_entities_detection_jobs()\n",
    "for job in list_result['EntitiesDetectionJobPropertiesList']:\n",
    "    print(f'Job ID: {job[\"JobId\"]}, Status: {job[\"JobStatus\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Download and Process the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = '48a33dda99b62aec30273a0d757a3d90'\n",
    "# Link to resutls file\n",
    "\n",
    "entities_results_S3Url = comprehend.describe_entities_detection_job(\n",
    "    JobId=job_id\n",
    ")['EntitiesDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "\n",
    "# S3 Uri\n",
    "#entities_results_S3Url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know where your results are, get a listing of your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files and folders in the bucket\n",
    "def s3_bucket_list_obj(bucket):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    for obj in bucket.objects.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files\n",
    "s3_bucket_list_obj(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3_name = 's3://' + bucket_name + '/'\n",
    "results_aws_filename = entities_results_S3Url.replace(s3_name, '')\n",
    "local_results_filename = 'Comprehend/outputs/entities.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "s3.download_file(bucket_name,\n",
    "                 results_aws_filename, \n",
    "                 local_results_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file\n",
    "import tarfile\n",
    "\n",
    "def extract_targz(targz_file, output_path = ''):\n",
    "    if targz_file.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(targz_file, \"r:gz\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "    elif targz_file.endswith(\"tar\"):\n",
    "        tar = tarfile.open(targz_file, \"r:\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'Comprehend/outputs/extracted'\n",
    "extract_targz(local_results_filename, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read JSON into a dictionary   \n",
    "input_file = output_path + '/output'\n",
    "entities = [json.loads(line) for line in open(input_file, 'r')]\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses entities data into a dataframe\n",
    "def parse_entities(data):\n",
    "    df = pd.DataFrame() # declare an empty dataframe\n",
    "    nested_json = 'Entities' # nested sub-dictiptionary to extract data from\n",
    "    # populate the dataframe\n",
    "    for line in data:\n",
    "        dt_temp = pd.DataFrame(line[nested_json])  # extract data from sub-dictionary\n",
    "        other_fields = list(line.keys())\n",
    "        other_fields.remove(nested_json) # remove nested fields        \n",
    "        for field in other_fields:  # add common fields\n",
    "            dt_temp[field] = line[field]\n",
    "        \n",
    "        df = df.append(dt_temp, ignore_index = True)\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the dataframe - this may take some time if you have >1,000 records\n",
    "entities_df = parse_entities(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "df = pd.read_csv(local_file_name, header = None, names = ['amazon_tweets'], dtype = 'str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A LIGHT LAST SEEN - US https://t.co/uKL2cS4ayd Jaynie Highsmith needs a happy ending but happy doesn't last forever. Will she repeat the same mistakes over &amp; over? #SecondChances #WomensFiction @Grace_Greene https://t.co/o43zSpujcP https://t.co/9F8uG20lfr\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeginOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>File</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.765808</td>\n",
       "      <td>A LIGHT LAST SEEN</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>amazon_tweets.csv</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.621316</td>\n",
       "      <td>US</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>amazon_tweets.csv</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>23.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.995827</td>\n",
       "      <td>https://t.co/uKL2cS4ayd</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>amazon_tweets.csv</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>47.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.999372</td>\n",
       "      <td>Jaynie Highsmith</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>amazon_tweets.csv</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>198.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.572987</td>\n",
       "      <td>@Grace_Greene</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>amazon_tweets.csv</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>212.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.994321</td>\n",
       "      <td>https://t.co/o43zSpujcP</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>amazon_tweets.csv</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>236.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>0.989828</td>\n",
       "      <td>https://t.co/9F8uG20lfr</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>amazon_tweets.csv</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BeginOffset  EndOffset     Score                     Text      Type  \\\n",
       "550          0.0       17.0  0.765808        A LIGHT LAST SEEN     TITLE   \n",
       "551         20.0       22.0  0.621316                       US  LOCATION   \n",
       "552         23.0       46.0  0.995827  https://t.co/uKL2cS4ayd     OTHER   \n",
       "553         47.0       63.0  0.999372         Jaynie Highsmith    PERSON   \n",
       "554        198.0      211.0  0.572987            @Grace_Greene    PERSON   \n",
       "555        212.0      235.0  0.994321  https://t.co/o43zSpujcP     OTHER   \n",
       "556        236.0      259.0  0.989828  https://t.co/9F8uG20lfr     OTHER   \n",
       "\n",
       "                  File  Line  \n",
       "550  amazon_tweets.csv   150  \n",
       "551  amazon_tweets.csv   150  \n",
       "552  amazon_tweets.csv   150  \n",
       "553  amazon_tweets.csv   150  \n",
       "554  amazon_tweets.csv   150  \n",
       "555  amazon_tweets.csv   150  \n",
       "556  amazon_tweets.csv   150  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_no = 150\n",
    "# Tweet text\n",
    "print(df.loc[record_no].item())\n",
    "# Resutls\n",
    "entities_df.query('Line == @record_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
